{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a6c33e2543d195b",
   "metadata": {},
   "source": [
    "Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "458216724f90e038",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T16:42:44.542930Z",
     "start_time": "2024-05-28T16:42:40.008903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0']\n",
      "List of physical GPU devices\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# setting the root path\n",
    "ROOT_DIR = os.path.join(os.getcwd(),\"..\",\"..\")\n",
    "DEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "from MetricTrackingCode import get_available_devices\n",
    "from MetricTrackingCode import MTrackingDataset\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import model as modellib, utils \n",
    "\n",
    "# setting the path to the mrcnn pre-trained weights\n",
    "COCO_WEIGHTS_PATH = os.path.join(ROOT_DIR,\"mrcnn\", \"mask_rcnn_coco.h5\")\n",
    "\n",
    "#checks if there are GPUs available\n",
    "print(get_available_devices())\n",
    "\n",
    "#show list of all GPU available\n",
    "print(\"List of physical GPU devices\")\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "print(len(gpu_devices))\n",
    "for gpu in gpu_devices:\n",
    "    print(\"here\")\n",
    "    print(' '*3,gpu)\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386acbe365e4fa9e",
   "metadata": {},
   "source": [
    "SETTING THE CONFIGURATION FILE FOR TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8220b9c0cd8a47be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T23:15:30.686696Z",
     "start_time": "2024-05-28T23:15:30.061692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.9\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "EPOCHS                         20\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  800\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               1000\n",
      "MAX_QUEUE                      10\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           None\n",
      "NUM_CLASSES                    4\n",
      "Name                           MTracking\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "RUN_EAGERLY                    False\n",
      "STEPS_PER_EPOCH                1000\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  False\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class MTackingConfig(Config):\n",
    "    Name = 'MTracking'\n",
    "    IMAGES_PER_GPU = 1\n",
    "    NUM_CLASSES = 1 + 3\n",
    "    EPOCHS = 20\n",
    "    MAX_GT_INSTANCES = 1000\n",
    "    DETECTION_MIN_CONFIDENCE = 0.9\n",
    "    USE_MINI_MASK = False  \n",
    "    RUN_EAGERLY = False\n",
    "    MAX_QUEUE=10\n",
    "\n",
    "config = MTackingConfig()\n",
    "config.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c12618e004ca0b",
   "metadata": {},
   "source": [
    "INITIALIZING THE DATAC CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb7b676b133ada03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset/train/labels/All_Annotations.json\n",
      "<class 'dict'>\n",
      "Dataset/train/labels/All_Annotations.json\n",
      "<class 'dict'>\n",
      "===========================================================\n",
      "Image Train Count: 41\n",
      "Class Train Count: 4\n",
      "===========================================================\n",
      "  0. BG                                                \n",
      "  1. Root                                              \n",
      "  2. Shoot                                             \n",
      "  3. Node                                              \n",
      "===========================================================\n",
      "Image val count: 41\n",
      "Class val count: 4\n",
      "===========================================================\n",
      "  0. BG                                                \n",
      "  1. Root                                              \n",
      "  2. Shoot                                             \n",
      "  3. Node                                              \n"
     ]
    }
   ],
   "source": [
    "MTracking_DIR = os.path.join(ROOT_DIR, \"Dataset\")\n",
    "\n",
    "train_Dataset = MTrackingDataset()\n",
    "val_Dataset = MTrackingDataset()\n",
    "\n",
    "#Loading the data\n",
    "train_Dataset.load_data(\"Dataset/train/labels/All_Annotations.json\",\"Dataset/train\")\n",
    "val_Dataset.load_data(\"Dataset/train/labels/All_Annotations.json\",\"Dataset/train\")\n",
    "\n",
    "#transfors data into container structure\n",
    "train_Dataset.prepare()\n",
    "val_Dataset.prepare()\n",
    "config.VALIDATION_STEPS = len(val_Dataset.image_ids)\n",
    "\n",
    "print(\"===========================================================\")\n",
    "print(\"Image Train Count: {}\".format(len(train_Dataset.image_ids)))\n",
    "print(\"Class Train Count: {}\".format(train_Dataset.num_classes))\n",
    "print(\"===========================================================\")\n",
    "\n",
    "for i, info in enumerate(train_Dataset.class_info):\n",
    "      print(\"{:3}. {:50}\".format(i, info['name']))\n",
    "        \n",
    "        \n",
    "print(\"===========================================================\")\n",
    "print(\"Image val count: {}\".format(len(val_Dataset.image_ids)))\n",
    "print(\"Class val count: {}\".format(val_Dataset.num_classes))\n",
    "print(\"===========================================================\")\n",
    "\n",
    "for i, info in enumerate(val_Dataset.class_info):\n",
    "    print(\"{:3}. {:50}\".format(i, info['name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5073d1af68aa54",
   "metadata": {},
   "source": [
    "SETTING UP THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bf452a06b35e396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ai/dev/Metric_Tracking/Mask_RCNN/samples/MetricTracking/../../mrcnn/mask_rcnn_coco.h5\n"
     ]
    }
   ],
   "source": [
    "model = modellib.MaskRCNN(mode=\"training\",\n",
    "                          config=config, \n",
    "                          model_dir=DEFAULT_LOGS_DIR)\n",
    "print(COCO_WEIGHTS_PATH)\n",
    "model.load_weights(COCO_WEIGHTS_PATH, by_name=True, \n",
    "                   exclude=[\"rpn_model\",\n",
    "                            \"mrcnn_class_logits\", \n",
    "                            \"mrcnn_bbox_fc\", \n",
    "                            \"mrcnn_bbox\", \n",
    "                            \"mrcnn_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f24dce0146a55f",
   "metadata": {},
   "source": [
    "TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a80eb5394403352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== PRINT HERE ====\n",
      "20\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "rpn_model              (Functional)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n",
      "n workers used: 12\n",
      "20\n",
      "<mrcnn.model.MaskRCNN object at 0x7f67d7748910>\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/mask_rcnn/roi_align_classifier/sub:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/mask_rcnn/roi_align_classifier/GatherV2_2:0\", shape=(None, 7, 7, 256), dtype=float32), dense_shape=Tensor(\"gradient_tape/mask_rcnn/roi_align_classifier/Shape_2:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/mask_rcnn/roi_align_classifier/sub_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/mask_rcnn/roi_align_classifier/GatherV2_5:0\", shape=(None, 7, 7, 256), dtype=float32), dense_shape=Tensor(\"gradient_tape/mask_rcnn/roi_align_classifier/Shape_3:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/mask_rcnn/roi_align_classifier/sub_2:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/mask_rcnn/roi_align_classifier/GatherV2_8:0\", shape=(None, 7, 7, 256), dtype=float32), dense_shape=Tensor(\"gradient_tape/mask_rcnn/roi_align_classifier/Shape_4:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/mask_rcnn/roi_align_classifier/sub_3:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/mask_rcnn/roi_align_classifier/GatherV2_11:0\", shape=(None, 7, 7, 256), dtype=float32), dense_shape=Tensor(\"gradient_tape/mask_rcnn/roi_align_classifier/Shape_5:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/mask_rcnn/ROI/Reshape_12:0\", shape=(6000,), dtype=int32), values=Tensor(\"gradient_tape/mask_rcnn/ROI/Reshape_11:0\", shape=(6000, 4), dtype=float32), dense_shape=Tensor(\"gradient_tape/mask_rcnn/ROI/Cast_1:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/mask_rcnn/roi_align_mask/sub:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/mask_rcnn/roi_align_mask/GatherV2_2:0\", shape=(None, 14, 14, 256), dtype=float32), dense_shape=Tensor(\"gradient_tape/mask_rcnn/roi_align_mask/Shape_2:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/mask_rcnn/roi_align_mask/sub_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/mask_rcnn/roi_align_mask/GatherV2_5:0\", shape=(None, 14, 14, 256), dtype=float32), dense_shape=Tensor(\"gradient_tape/mask_rcnn/roi_align_mask/Shape_3:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/mask_rcnn/roi_align_mask/sub_2:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/mask_rcnn/roi_align_mask/GatherV2_8:0\", shape=(None, 14, 14, 256), dtype=float32), dense_shape=Tensor(\"gradient_tape/mask_rcnn/roi_align_mask/Shape_4:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/mask_rcnn/roi_align_mask/sub_3:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/mask_rcnn/roi_align_mask/GatherV2_11:0\", shape=(None, 14, 14, 256), dtype=float32), dense_shape=Tensor(\"gradient_tape/mask_rcnn/roi_align_mask/Shape_5:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"===== PRINT HERE ====\")\n",
    "print(config.EPOCHS)\n",
    "model.train(train_Dataset,val_Dataset,\n",
    "            learning_rate=config.LEARNING_RATE,\n",
    "            epochs=config.EPOCHS,\n",
    "            layers = 'heads',\n",
    "            max_queue_size=config.MAX_QUEUE,\n",
    "            use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [],
   "id": "b7fbc39bd3d67378"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [],
   "id": "4d487d09c80186"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
